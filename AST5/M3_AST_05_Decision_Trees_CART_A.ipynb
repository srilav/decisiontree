{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M3_AST_05_Decision_Trees_CART_A.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S8MbYo0x3MmS"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 5: Decision Trees and CART Algorithm "]},{"cell_type":"markdown","metadata":{"id":"9VoaylChBGuP"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"3MaUJgglBIdk"},"source":["At the end of the experiment, you will be able to\n","\n","* know the concept of decision tree algorithm\n","* know how to visualize a decision tree and make illustrations from it\n","* understand the classification and regression tree (CART) algorithm\n","* know about the basic mathematics behind the predictions and classifications made by decision trees\n","* know what is hyperparameter regularization"]},{"cell_type":"markdown","metadata":{"id":"xrpWSbn3BMdv"},"source":["### Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"nYH_PqM_BVd8"},"source":["Decision Trees are supervised Machine Learning algorithms that can perform both classification and regression tasks and even multioutput tasks. They can handle complex datasets. As the name shows, it uses a tree-like model to make decisions in order to classify or predict according to the problem. It is an ML algorithm that progressively divides datasets into smaller data groups based on a descriptive feature until it reaches sets that are small enough to be described by some label.\n","\n","The importance of decision tree algorithm is that it has many applications in the real world. For example:\n","\n","1. In the Healthcare sector: To develop Clinical Decision Analysis tools which allow decision-makers to apply for evidence-based medicine and make objective clinical decisions when faced with complex situations.\n","2. Virtual Assistants (Chatbots): To develop chatbots that provide information and assistance to customers in any required domain.\n","3. Retail and Marketing: Sentiment analysis detects the pulse of customer feedback and emotions and allows organizations to learn about customer choices and drives decisions.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eXvRD-s1Sp2Z"},"source":["#### How can an algorithm be represented as a tree?"]},{"cell_type":"markdown","metadata":{"id":"WkRGdQfVSp2b"},"source":["![Image](https://i.ibb.co/rwBRM7B/decistion-tree.jpg)\n","\n","$\\hspace{8cm} \\text {Figure 1: Basic Structure of Decision Tree}$\n","\n","For this, let us see the basic example of the [titanic dataset](https://data.world/nrippner/titanic-disaster-dataset) which predicts whether a passenger survives or not. The below tree uses 3 attributes from the dataset, namely sex, age, and sibsp (Number of Siblings/Spouses Aboard).\n","\n","![Image](https://miro.medium.com/max/360/1*XMId5sJqPtm8-RIwVVz2tg.png)\n","\n","$\\hspace{8cm} \\text {Figure 2: Decision tree using example}$\n","\n","A decision tree is drawn upside down with its root at the top. In the image on the left (is age $>$ 9.5), the bold text in black represents a condition/internal node, based on which the tree splits into branches/ edges. The end of the branch that doesn’t split anymore is the decision/leaf, in this case, whether the passenger died or survived, represented as red and green text respectively.\n","\n","To know more about, decision trees, click [here](https://blog.paperspace.com/decision-trees/).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pHo_nkR0Sp2e"},"source":["### Notebook Overview"]},{"cell_type":"markdown","metadata":{"id":"6Wihx5MFSp2h"},"source":["The notebook contains the following topics:\n","\n","- Different methods of visualization of decision tree: Classification and Regression\n","- Training of decision trees\n","- The mathematics of decision tree for both classification and regression problems\n","- Overview of CART algorithm\n","- Regularization of Hyperparameters\n","- Visualization of decision boundaries (hyper-plane) with default hyperparameters and after regularization of hyperparameters\n","- Limitations of decision tree algorithm"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"M3_AST_05_Decision_Trees_CART_A\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")  \n","    \n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VI9tcPmIU2o3"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"KbT4cMfzSp2o"},"source":["# Install dtreeviz library\n","!pip -qq install dtreeviz                      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKrMcAub09Pv"},"source":["import numpy as np \n","from matplotlib import pyplot as plt\n","from matplotlib.colors import ListedColormap                                    # to import color map                                          \n","from sklearn import datasets\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor          # to import DT classifier and Regressor\n","import graphviz                                                                 # to import graphviz \n","from dtreeviz.trees import dtreeviz                                             # to import dtreeviz \n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9t88BvrU7Pd"},"source":["### Training and Visualizing a Decision Tree"]},{"cell_type":"markdown","metadata":{"id":"qcv_xO3gVKuM"},"source":["The decision trees can be divided, with respect to the target values, into:\n","\n","- Classification trees: used to classify samples, assign to a limited set of values - classes. In Scikit-Learn it is `DecisionTreeClassifier`.\n","- Regression trees: used to assign samples into numerical values within the range. In Scikit-Learn it is `DecisionTreeRegressor`.\n","\n","To understand Decision Trees, let’s just build one and take a look at how it makes predictions. The  following  code  trains  a  DecisionTreeClassifier  on  the  iris  dataset:"]},{"cell_type":"code","metadata":{"id":"WlXuqwPNXDEa"},"source":["# Prepare the data into X (predictor) and Y (target), considering sepal length and width\n","iris = datasets.load_iris()\n","X = iris.data[:, 2:]\n","# YOUR CODE HERE to create 'y' using iris.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-ATUnSRXpsc"},"source":["# Fit the DT classifier with max_depth = 2\n","clf = DecisionTreeClassifier(max_depth = 2, random_state=1234)\n","model = clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dd1YHJuJYGQO"},"source":["#### Text Representation\n","\n","Visualizing the Decision tree in text using `text.export_text`. This is important when we want to obtain the model information in a text file."]},{"cell_type":"code","metadata":{"id":"AjbThBGTX0cH"},"source":["text_representation = tree.export_text(clf)\n","# Display result\n","# YOUR CODE HERE to show 'text_representation'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9MRMTLVY3Io"},"source":["If you want to save it to the file, it can be done with the following code:"]},{"cell_type":"code","metadata":{"id":"5sYY3gVUYEZB"},"source":["# Save in figure\n","with open(\"decision_tree.log\", \"w\") as f_out:\n","    f_out.write(text_representation)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SO87jz1AZKpO"},"source":["#### Visualize Tree with `plot_tree`\n","\n","This feature requires matplotlib library to be installed. It allows us to easily visualize the tree into a figure (without intermediate exporting to graphviz). You can also refer the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) to learn more about `plot_tree`."]},{"cell_type":"code","metadata":{"id":"cQ103fwdY8tb"},"source":["# Visualize tree\n","fig = plt.figure(figsize=(10,8))\n","_ = tree.plot_tree(clf, \n","                   feature_names=iris.feature_names,  \n","                   class_names=iris.target_names,\n","                   filled=True)                           # filled = True uses color coding for majority of classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NUry0gta-Gx"},"source":["# Save the figure to the .png file\n","fig.savefig(\"decision_tree.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UcEVvZnLbYPB"},"source":["#### How Decision Tree Makes Prediction"]},{"cell_type":"markdown","metadata":{"id":"Fr6Ximy1Sp3B"},"source":["In the above example, we see how the Decision tree makes predictions. In this, we need to classify a flower. At depth 0, at the top (root node), this node asks the condition whether the flower’s sepal length $\\leq$ 2.45 cm. If it is, then we move down to the root’s left child node (depth 1, left). Here, it's a leaf node (means it does not have any child nodes), so it does not ask any questions: you can simply look at the predicted class for that node and the  Decision Tree predicts that the flower is an Iris-Setosa (class=setosa).\n","\n","Now, in another case, when the sepal length of a flower is $\\geq$ 2.45cm. We then move down to right the child node (depth 1, right), which is not a leaf node, and ask if sepal width is $\\leq$ 1.75cm. If it is, then the flower is Iris-Versicolor (depth 2, left). If not, it is surely, Iris-Virginica (depth 2, right).\n","\n","On each node, the `samples` represent the number of training instances. For example, 54 samples have sepal length $\\geq$ 2.45cm and sepal width $\\leq$ 1.75cm (depth 2, left). The node's `values` show how many training instances of each class this node applies. For example, at depth 2, right, applies 0 to Iris-Setosa, 1 to Iris-Versicolor, and 45 to Iris-Virginica.\n","\n","At last, the `gini-impurity` measures the node's impurity, if a node is `pure` (gini = 0), means that all the instances belong to a single class. Example, depth 1, left node. "]},{"cell_type":"markdown","metadata":{"id":"tiiXkASuSp3C"},"source":["##### Gini Impurity and Entropy"]},{"cell_type":"markdown","metadata":{"id":"6i3LhO3rSp3D"},"source":["In a decision tree, by default `gini_impurity` is used. But, we can use `entropy` by changing the `criterion` hyperparameter to \"`entropy`\". The gini impurity is one of the methods used in decision tree algorithms to decide the optimal split from a root node and subsequent splits. \n","\n","The entropy came from thermodynamics, where if entropy approaches 0 that means the molecules are still and well-ordered. In ML, it is used to measure the impurity: a set's entropy 0 means that all the instances belong to the same class.\n","\n","*Equation for calculating the Gini-impurity:*\n","\n","$G_i =$ $1 - \\sum_{k=1}^{n} p_{i,k}^2$ \n","\n","The above equation shows how the training algorithm computes the gini-score $G_i$ of the $i^{th}$ node. \n","\n","For example, the *depth-2 left node* has a gini score equal  to $1 – (0/54)^2 – (49/54)^2 – (5/54)^2 ≈ 0.168$.\n","\n","*Equation for calculating the Entropy:*\n","\n","$H_i =$ $- \\sum_{i=1}^{n} p_{i,k} \\log_2 (p_{i,k})$, $\\hspace{0.5cm}where, p_{i,k} \\neq 0$\n","\n","The above equation shows the definition  of  the  entropy  of  the  $i^{th}$  node.\n","\n","For  example, given above, the *depth-2 left node* has an entropy equal to $- \\frac{49}{54} \\log_2 (\\frac{49}{54}) - \\frac{5}{54} \\log_2 (\\frac{5}{54}) \\approx 0.445$\n","\n","Where, $k$ is class of the problem. In this example, there are three classes, Iris-versicolor, Iris-Setosa, and Iris-Virginica."]},{"cell_type":"markdown","metadata":{"id":"aedC-2UGSp3F"},"source":["#### Estimating Class Probabilities"]},{"cell_type":"markdown","metadata":{"id":"6B9SyG9jSp3G"},"source":["The instance probability denoted by $p_{i,k}$ denotes the probability of an instance belongs to a class $k$. \n","\n","For example, we have a flower whose sepal length is 5cm and sepal width is 1.5cm (depth 2, left node). The following probabilities: 0% for Iris-Setosa $(\\frac{0}{54})$, 90.74% for Iris-Versicolor $(\\frac{49}{54})$, and 9.25% for Iris-Verginica $(\\frac{5}{54})$. And this will predict the class as Iris-Versicolor. "]},{"cell_type":"code","metadata":{"id":"0iXqQvXJSp3G"},"source":["# Predicting probability of a class\n","clf.predict_proba([[5, 1.5]])          "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8lSowh_YSp3H"},"source":["# Predicting a class\n","# YOUR CODE HERE to make prediction on [[5, 1.5]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0crxUzXSp3I"},"source":["### The CART Training Algorithm"]},{"cell_type":"markdown","metadata":{"id":"1IM-dqeGSp3J"},"source":["Scikit-Learn  uses  the  Classification  And  Regression  Tree  (CART)  algorithm  to  train Decision Trees (also called “growing” trees). CART constructs binary trees using the feature and threshold that yield the largest information gain or minimum gini-impurity at each node.\n","\n","*Cart Cost Function For Classification*\n","\n","$J_{k,t_k} =$ $\\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right}$\n","\n","\\begin{equation*}\n","Where\\begin{cases}\n","         G_{left/right} \\hspace{0.25cm} \\text{measures the  impurity  of  the  left/right  subset}, \\\\\n","         m_{left/right} \\hspace{0.25cm} \\text{is  the  number  of  instances  in  the  left/right  subset}  \\\\\n","     \\end{cases}\n","\\end{equation*}\n","\n","Equation shows that the algorithm first the splits the training subsets using a single feature $k$ and a limiting value or condition $t_k$. The algorithm works as follows:\n","\n","- It creates different pairs of $(k,t_k)$. It searches for the pair that produces the purest subset (i.e.minimum gini-impurity) weighted by their size.\n","\n","Once it splits the training set in  two, it splits the subsets using the same algorithm and this iteration will be done until it reaches the maximum depth (i.e., `max_depth` hyperparameter). A  few  other  hyperparameters control   additional  stopping   conditions (`min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes`).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IIXelwLbSp3U"},"source":["### Regularization Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"O9lXP1cBSp3a"},"source":["Decision tree does not make any prior assumptions of training data (unlike linear models, which assume the data as linear).  If left unconstrained, the tree will closely fit itself according to the training instances, and most likely lead to overfitting (means the model will perform poorly on the test dataset). So, to avoid overfitting the training data, the hyperparameters need to be restricted and passed during the modeling. This process is what is known as Regularization Hyperparameters. \n","\n","In  Scikit-Learn,  this  is  controlled  by  the `max_depth`  hyperparameter  (the  `default  value`  is  `None`,  which  means  unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting. The `DecisionTreeClassifier` class has a few other parameters that similarly restrict the  shape  of  the  Decision  Tree:  `min_samples_split` (the  minimum  number  of  samples a node must have before it can be split), `min_samples_leaf` (the minimum number   of   samples   a   leaf   node   must   have),   `min_weight_fraction_leaf`   (same   as min_samples_leaf  but  expressed  as  a  fraction  of  the  total  number  of  weighted instances), `max_leaf_nodes` (maximum  number  of  leaf  nodes),  and  `max_features` (maximum number of features that are evaluated for splitting at each node). Increasing  `min_*`  hyperparameters  or  reducing  `max_*`  hyperparameters  will  regularize  the model."]},{"cell_type":"markdown","metadata":{"id":"7kcgKcBfSp3b"},"source":["In the example below, we will visualize the decision tree, one by regularizing hyperparameter and one without any restrictions.  "]},{"cell_type":"code","metadata":{"id":"58y7KMn5Sp3c"},"source":["# Load iris dataset and define variables with sepal length and width\n","iris = datasets.load_iris()\n","X = iris.data[:, 2:] \n","# YOUR CODE HERE to create 'y' using iris.target\n","\n","# First tree without restrictions\n","tree_clf = DecisionTreeClassifier(random_state=42)\n","tree_clf.fit(X, y)\n","\n","# Second tree with hyperparameters\n","# YOUR CODE HERE to create 'tree_clf2' using DecisionTreeClassifier with (max_depth =2, min_samples_leaf =1, min_samples_split = 2, random_state=2)\n","# YOUR CODE HERE to fit 'tree_clf2' on (X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHGevdGrSp3e"},"source":["# Define a function for plotting decision boundary\n","def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris = True, legend=False, plot_training=True):\n","  \n","    # define array for x1 and x2 axes\n","    x1s = np.linspace(axes[0], axes[1], 100)                                                                \n","    # YOUR CODE HERE to create 'x2s' with (axes[2], axes[3], 100) \n","    \n","    # make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids                                                               \n","    x1, x2 = np.meshgrid(x1s, x2s)         \n","\n","    # the numpy.ravel() functions returns contiguous flattened array(1D array with all the input-array elements and with the same type as it)                                                                 \n","    X_new = np.c_[x1.ravel(), x2.ravel()]                                       \n","    # predict and reshape the y_pred according to x\n","    y_pred = clf.predict(X_new).reshape(x1.shape)  \n","\n","    # module is used for mapping numbers to colors or color specification conversion in a 1-D array of colors also known as colormap\n","    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])               \n","    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)                         \n","  \n","    if plot_training:\n","        # plot Setosa in yellow\n","        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")       \n","        # plot Versicolor in blue  \n","        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")   \n","        # plot Virginica in green\n","        # YOUR CODE HERE to plot Virginica using (\"g^\", label=\"Iris virginica\")    \n","        plt.axis(axes)\n","\n","    if iris:\n","        # define x_axes label\n","        plt.xlabel(\"Sepal length\", fontsize=14)                                 \n","        # define y_axes label\n","        # YOUR CODE HERE to set ylabel as \"Sepal width\" with fontsize=14                                  \n","    \n","    if legend:\n","        plt.legend(loc=\"lower right\", fontsize=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxvhbBPtSp3f"},"source":["To know more about Listedcolormap and meshgrid in the above code, click [here](https://stackoverflow.com/questions/44443993/matplotlib-colors-listedcolormap-in-python)."]},{"cell_type":"code","metadata":{"id":"15k2xWeMSp3g"},"source":["# Plot both the decision tree\n","plt.figure(figsize=(8, 4))\n","\n","# call the plot_decision_boundary function for tree_clf\n","plot_decision_boundary(tree_clf, X, y)\n","plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n","plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n","plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n","plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n","plt.title(\"No restrictions\", fontsize=16)\n","plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n","plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n","plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n","\n","plt.figure(figsize=(8, 4))\n","\n","# call the plot_decision_boundary function for tree_clf2\n","plot_decision_boundary(tree_clf2, X, y)\n","plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n","# YOUR CODE HERE to plot dashed line with ([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n","# YOUR CODE HERE to plot dotted line with ([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n","# YOUR CODE HERE to plot dotted line with ([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n","plt.title(\"Regularizing Hyperparameters\", fontsize=16)\n","# YOUR CODE HERE to add \"Depth=0\" text\n","# YOUR CODE HERE to add \"Depth=1\" text\n","# YOUR CODE HERE to add \"(Depth=2)\"text\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXsrHQETSp3h"},"source":["From the above figure, we can see that, when there is no restriction of hyperparameters in decision tree, it can adjust itself according to the training dataset (overfitting problem). While, on the other hand, where there is the regularization of hyperparameters, the model will probably generalize better."]},{"cell_type":"markdown","metadata":{"id":"OxptFod4Sp3i"},"source":["### Decision Tree: Regression"]},{"cell_type":"markdown","metadata":{"id":"Xxs71Id3Sp3j"},"source":["Decision  Trees  are  also  capable  of  performing  regression  tasks.  Below, present the `DecisionTreeRegressor` visualization methods from Scikit-Learn package. Here, in this section, we use the Boston dataset to create a decision tree regressor."]},{"cell_type":"code","metadata":{"id":"QqlLTgphSp3k"},"source":["# Prepare the data \n","boston = datasets.load_boston()\n","X = boston.data\n","# YOUR CODE HERE to create 'y' using boston.target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVPMeUYcSp3m"},"source":["To keep the size of the tree small, set max_depth = 3."]},{"cell_type":"code","metadata":{"id":"d6IdvbkoSp3n"},"source":["# Fit the regressor, set max_depth = 3\n","dt = DecisionTreeRegressor(max_depth=3, random_state=1234)\n","model = dt.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwMkENDQSp3n"},"source":["Visualizing Decision Tree Regression same as classifier in previous topic. "]},{"cell_type":"code","metadata":{"id":"WDANNeiKSp3p"},"source":["text_representation = tree.export_text(dt)\n","# YOUR CODE HERE to show 'text_representation'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOVRIdDISp3t"},"source":["# Visualize tree using plot_tree\n","fig = plt.figure(figsize=(15,10))\n","_ = tree.plot_tree(dt, feature_names=boston.feature_names, filled=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FwxtbAluSp3u"},"source":["# Visualize tree using graphviz\n","dot_data = tree.export_graphviz(dt, out_file=None, \n","                                feature_names=boston.feature_names,  \n","                                filled=True)\n","graphviz.Source(dot_data, format=\"png\") "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWMtLVM2Sp3z"},"source":["# Visualize tree using dtreeviz\n","vizualize = dtreeviz(dt, X, y,\n","                target_name=\"target\",\n","                feature_names=boston.feature_names)\n","vizualize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ANvZTd3KSp30"},"source":["From the above `plot_tree` and `graphviz` diagram, we can see that the decision tree for regression is similar to that of classification. But with only one difference, here it is predicting a value instead of a class. So, suppose we want to make a prediction of a new instance which is RM = 7 and NOX = 0.6. Starting from the root node, we end up at the leaf node that predicts value = 33.35. This value is just the average of 43 training instances that fall into this leaf node. This prediction results in mean squared error (MSE) = 20.11 over 43 instances. \n","\n","Thus, the target value predicted in each region is the average target value of the instances in that region.  The algorithm splits each region in a way that makes most training instances as close as possible to that predicted value.\n","\n","The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training  set  in  a  way  that  minimizes  the  MSE. \n","\n","*Cart Cost Function For Regression*\n","\n","$J_{k,t_k} =$ $\\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}$\n","\n","\\begin{equation*}\n","Where\\begin{cases}\n","         MSE_{node} \\hspace{0.25cm} \\sum_{i \\in node} (\\hat{y} - y^{(i)})^2, \\\\\n","         \\hat{y} = \\frac{1}{m_{node}} \\sum_{i \\in node} y^{(i)}  \\\\\n","     \\end{cases}\n","\\end{equation*}\n","\n","As we saw in classification problem, decision trees in regression with no regularization on hyperparameters are also prone to overfitting. From the graph obtained by running the below code, we see predictions obtained with `max_depth = 8` on left and on the right without any regularization. "]},{"cell_type":"code","metadata":{"id":"tq9FQKgVSp34"},"source":["# Define plot regression function\n","\n","def plot_regression_predictions(tree_reg, X, y, axes=[0.3, 1, 0, 60], ylabel=\"$y$\"):\n","    # creating the x-axes grid in array\n","    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)                                \n","    # define y\n","    y_pred = tree_reg.predict(x1)                                                         \n","    plt.axis(axes)\n","    plt.xlabel(\"$x_1$\", fontsize=18)\n","    \n","    if ylabel:                              \n","        plt.ylabel(ylabel, fontsize=18, rotation=0)                                    \n","    \n","    plt.plot(X, y, \"b.\")\n","    # plot y hat (predicted values) in red line in both graphs\n","    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")                         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DW1PO_9Sp35"},"source":["# Take index value 4 i.e, NOX\n","X = boston.data[:,4:5]                                          \n","y = boston.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qElP7yGwSp35"},"source":["# Define model with no hyperparameter\n","dt = DecisionTreeRegressor(random_state=1234)                                       \n","dt.fit(X, y)\n","\n","# Define model with maximum depth = 8\n","dt2 = DecisionTreeRegressor(max_depth=8, random_state=1234)                         \n","# YOUR CODE HERE to fit 'dt2' on (X, y)\n","\n","fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n","plt.sca(axes[0])\n","\n","# Plot decision boundary\n","plot_regression_predictions(dt2, X, y)\n","\n","for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n","    # Plot the fit regression line\n","    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n","\n","plt.legend(loc=\"upper center\", fontsize=18)\n","plt.title(\"max_depth=8\", fontsize=14)\n","\n","plt.sca(axes[1])\n","# Plot dcision boundary\n","plot_regression_predictions(dt, X, y, ylabel=None)\n","\n","for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n","    # Plot the fit regression line\n","    # YOUR CODE HERE for regression fit line\n","\n","# YOUR CODE HERE to set title as \"max_depth=No restriction\" with fontsize=14\n","plt.text(0.3,1, \"X1 = NOX feature\", fontsize=10)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYv6fccqSp35"},"source":["The one without any hyperparameters setting is obviously overfitting the training instances and just by restricting max_depth = 8 results in a much more reasonable model."]},{"cell_type":"markdown","metadata":{"id":"f_g1jVuBSp36"},"source":["### Limitations of Decision Tree Algorithm"]},{"cell_type":"markdown","metadata":{"id":"8xeHeSuxSp38"},"source":["The major limitations of decision tree approaches for data analysis are:\n","\n","- Provide less information on the relationship between the predictors and the target variable (unlike, in linear models, we get a mathematical relation between x and y as y = $\\theta$ x + c).\n","- Biased toward predictors with more variance.\n","- Can have issues with highly collinear predictors.\n","- Can have poor prediction accuracy for responses with low sample sizes.\n","- Trees can be unstable because small variations in the data might result in a completely different tree being generated.\n","- For classification, if some classes dominate, it can create biased trees. It is therefore recommended to balance the dataset prior to fitting."]},{"cell_type":"markdown","metadata":{"id":"H4H9ISEnSp38"},"source":["### Theory Questions"]},{"cell_type":"markdown","metadata":{"id":"6Fgl0tY-Sp39"},"source":["1. What  is  the  approximate  depth  of  a  Decision  Tree  trained  (without  restrictions)\n","on a training set with 1 million instances?\n","\n"," The depth of a well-balanced binary tree containing $m$ leaves is equal to $\\log_2(m)$, rounded up. A binary Decision Tree (one that makes only binary decisions, as is the case of all trees in Scikit-Learn) will end up more or less well balanced at the end of training, with one leaf per training instance if it is trained without restrictions. Thus, if the training set contains one million instances, the Decision Tree will have a depth of $\\log_2(10^6) ≈ 20$ (actually a bit more since the tree will generally not be perfectly well balanced).\n","\n","2. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?\n","\n"," If a Decision Tree is overfitting the training set, it may be a good idea to decrease max_depth, since this will constrain the model, regularizing it.\n","\n","3. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n","\n"," Decision Trees don’t care whether or not the training data is scaled or centered; that’s one of the nice things about them. So if a Decision Tree underfits the training set, scaling the input features will just be a waste of time.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["#@title A node’s Gini impurity is: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"generally lower than its parent’s\", \"generally greater than its parent's\", \"always lower than its parent's\", \"always greater than its parent's\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}