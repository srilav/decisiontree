{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"M3_AST_06_Ensemble_Learning_and_Random_Forests_B.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"X2k4DiTPPByL"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 6: Ensemble Learning and Random Forests"]},{"cell_type":"markdown","metadata":{"id":"O9e-oXO0PByO"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"4WnoQlmgPByO"},"source":["At the end of the experiment, you will be able to\n","\n","* understand Ensemble learning and Ensemble methods\n","* perform Voting Classifier and Bagging Classifier using Scikit-Learn package\n","* understand the concept of Random Forest\n","* perform classification using RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"id":"vEMf2D01PByO"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"BR00Xe7DPByP"},"source":["Instead of using a single predictor if we aggregate the predictions of a group of predictors (such as classifiers or regressors), we will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble, thus, this technique is called **Ensemble Learning**, and an Ensemble Learning algorithm is called an Ensemble method.\n","\n","The most popular Ensemble methods include: \n","* voting classifier\n","* bagging\n","* boosting \n","\n","Here, we will consider only voting classifier and bagging as well as an ensemble of Decision Trees called Random Forests."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"M3_AST_06_Ensemble_Learning_and_Random_Forests_B\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")  \n","    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/heart_failure_clinical_records_dataset.csv\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IyWhsz5JPByP"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"FdojBodnPByP"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier as KNN\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kCzkuBePByQ"},"source":["### Voting Classifier"]},{"cell_type":"markdown","metadata":{"id":"LVFuMooGPByQ"},"source":["In this ensemble method, we train the training sample on different algorithms and aggregate their predictions.\n","\n","Suppose we have trained a few classifiers, each one achieving about 80% accuracy. We may have a Logistic Regression classifier, an SVM classifier, a K-Nearest Neighbors classifier, and perhaps a few more as shown in the figure below.\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Voting_classifier_1.jpg\" width= 600 px/>\n","</center>\n","<br><br>\n","\n","Then to create an even better classifier we aggregate the predictions of each classifier and predict the class that gets the most votes as shown in the figure below. This majority-vote classifier is called a **hard voting** classifier.\n","\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Voting_classifier_prediction.png\" width= 600 px/>\n","</center><br><br>\n","\n","Surprisingly, it is found that the voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner (performs only slightly better than random guessing), the ensemble can still be a strong learner and achieve high accuracy, provided there are a sufficient number of weak learners and they are sufficiently diverse.\n","\n","We can use Scikit-Learn's **soft voting** to predict the class with the highest class probability, averaged over all the individual classifiers, provided that all the classifiers are able to estimate class probabilities (i.e., they have a `predict_proba()` method). It often achieves higher performance than hard voting because it gives more weight to highly confident votes."]},{"cell_type":"markdown","metadata":{"id":"TLfFPgcuPByR"},"source":["**Exercise 1:** Generate a dataset having 4 classes and compare the accuracy for different models when used individually as well as when used within the Voting classifier."]},{"cell_type":"code","metadata":{"id":"guKZtInoPByR"},"source":["# Generate sample data\n","X, y = datasets.make_blobs(n_samples=300,                   # total number of points equally divided among clusters\n","                           centers=4,                       # number of centers to generate\n","                           cluster_std=1.5,                 # standard deviation of the clusters\n","                           random_state=0                   # determines random number generation for reproducible output\n","                                                            # across multiple function calls\n","                           )\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TOGX7k9PByS"},"source":["# Splitting the Dataset \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.30, random_state= 123)\n","\n","# Instantiate LogisticRegression, KNN, SVC and VotingClassifier\n","log_clf = LogisticRegression()\n","knn_clf = KNN()\n","svm_clf = SVC(probability= True)\n","classifiers = [('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)]\n","\n","voting_clf = VotingClassifier(estimators= classifiers, voting='soft')\n","\n","for clf in (log_clf, knn_clf, svm_clf, voting_clf):\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nu5xiVUDPByS"},"source":["From the above result, we can see VotingClassifier has performed a little better than all of the other classifiers."]},{"cell_type":"markdown","metadata":{"id":"r_fAaR_NPByS"},"source":["### Bagging and Pasting"]},{"cell_type":"markdown","metadata":{"id":"5Z14kKy4PByT"},"source":["In this ensemble method, instead of using different training algorithms, we use the same training algorithm to train on different random subsets of the training set.\n","\n","When sampling is performed \n","* with replacement, this method is called **bagging** (short for bootstrap aggregating).\n","* without replacement, it is called **pasting**. \n","\n","In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor as shown in the figure below.\n","<br><br>\n","<center>\n","<img src=\"https://i2.wp.com/thecleverprogrammer.com/wp-content/uploads/2020/07/un-115.png?resize=839%2C425&ssl=1\" width= 600 px/>\n","</center>\n","<br><br>\n","\n","Once all predictors are trained, the ensemble can make a prediction for a new\n","instance by simply aggregating the predictions of all predictors. \n","\n","The aggregation function is typically \n","* the statistical mode (i.e., the most frequent prediction) for classification, or \n","* the average for regression. \n","\n","Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set. "]},{"cell_type":"markdown","metadata":{"id":"-DRWjvTwPByT"},"source":["**Exercise 2:** On the above created dataset perform classification using BaggingClassifier with DecisionTreeClassifier as base estimator."]},{"cell_type":"code","metadata":{"id":"Pypgiao5PByT"},"source":["dt = DecisionTreeClassifier()\n","bag_clf = BaggingClassifier( base_estimator=dt,     # base estimator to fit on random subsets of the dataset\n","                            n_estimators=300,       # number of base estimators in the ensemble\n","                            max_samples=100,        # number of samples to draw from X to train each base estimator. \n","                                                    # If int, then draw max_samples samples.\n","                                                    # If float, then draw (max_samples * X.shape[0]) samples.\n","                            bootstrap=True,         # whether samples are drawn with replacement\n","                            random_state= 123       # seed used by the random number generator to ensure reproducibility\n","                            )\n","# YOUR CODE HERE to fit 'bag_clf' on (X_train, y_train)\n","y_pred = bag_clf.predict(X_test)\n","print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KGW5bp_4PByT"},"source":["#### Out-of-Bag Evaluation"]},{"cell_type":"markdown","metadata":{"id":"E98iucvrPByU"},"source":["With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default, a `BaggingClassifier` samples $m$ training instances with replacement (`bootstrap=True`), where $m$ is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called **out-of-bag (oob) instances**. Note that they are not the same 37% for all predictors. \n","\n","Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. We can evaluate the ensemble itself by averaging out the oob evaluations of each predictor as shown in the figure below.\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/OOB_evaluation.png\" width= 600 px/>\n","</center>\n","<br><br>\n","\n","In Scikit-Learn, we can set `oob_score = True` when creating a `BaggingClassifier` to request an automatic oob evaluation after training."]},{"cell_type":"markdown","metadata":{"id":"jEc_K8ccPByU"},"source":["**Exercise 3:** Compute the OOB score for BaggingClassifier mentioned in Exercise 2."]},{"cell_type":"code","metadata":{"id":"TtmRueJHPByU"},"source":["dt = DecisionTreeClassifier()\n","bag_clf = BaggingClassifier( base_estimator=dt, \n","                            n_estimators=300, \n","                            max_samples=100, \n","                            bootstrap=True, \n","                            oob_score=True,              # use out-of-bag samples to estimate the generalization error\n","                            random_state= 123\n","                            )\n","bag_clf.fit(X_train, y_train)\n","# YOUR CODE HERE to create 'y_pred' using bag_clf to predict(X_test)\n","print(f\"OOB score: {bag_clf.oob_score_}\")\n","print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"huKPkadcPByU"},"source":["According to the above oob evaluation score, this BaggingClassifier is likely to achieve about 77% accuracy on the test set. And we get above 80% accuracy on the test set that is close enough."]},{"cell_type":"markdown","metadata":{"id":"eaEIJDkVPByU"},"source":["### Random Forests"]},{"cell_type":"markdown","metadata":{"id":"tFv5mfX1PByV"},"source":["A random forest is a collection of decision trees whose results are aggregated into one final result. Random Forest  is a supervised classification algorithm. There is a direct relationship between the number of trees in the forest and the results it can get: the larger the number of trees, the more accurate the result. But here creating the forest is not the same as constructing the decision tree with the information gain or gain index approach.\n","\n","The difference between the Random Forest algorithm and the decision tree algorithm is that in Random Forest, the process of finding the root node and splitting the feature nodes will run randomly.\n","\n","\n","**Why Random Forest algorithm?**  Overfitting is one critical problem that may make the results worse, but for Random Forest algorithm, if there are enough trees in the forest, the classifier won’t overfit the model. The advantage is the classifier of Random Forest can handle missing values and can be used to model the categorical values.\n","\n","\n","**How does Random Forest algorithm work?** There are two stages in Random Forest algorithm, one is random forest creation, the other is to make a prediction from the random forest classifier created in the first stage. \n","\n","**Steps:**\n","\n","1. Randomly select “k” features from total “m” features where k << m as shown in the figure below\n","2. Among the “k” features, calculate the node “d” using the best split point\n","3. Split the node into leaf nodes using the best split\n","4. Repeat the 1 to 3 steps until “l” number of nodes has been reached\n","5. Build forest by repeating steps 1 to 4 for “n” number times to create “n” number of trees.\n","6. Take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome (target)\n","7. Calculate the votes for each predicted target\n","8. Consider the high voted predicted target as the final prediction from the random forest algorithm\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Random_forests.png\" width = 600 px/>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"EhWz8d7wPByV"},"source":["The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model.\n","\n","With a few exceptions, a `RandomForestClassifier` has all the hyperparameters of a `DecisionTreeClassifier` plus all the hyperparameters of a `BaggingClassifier` to control the ensemble itself. \n","\n","To Know more about Random Forest classifier parameters, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."]},{"cell_type":"markdown","metadata":{"id":"WltH5UqoPByV"},"source":["**Exercise 4:** On the above created dataset perform classification using RandomForestClassifier. Also provide its equivalent, using BaggingClassifier."]},{"cell_type":"code","metadata":{"id":"ijvhhnApPByV"},"source":["rf_clf = RandomForestClassifier(n_estimators=300,           # number of trees in the forest\n","                                max_leaf_nodes=16,          # grow trees with max_leaf_nodes, selected by \n","                                                            # relative reduction in impurity. \n","                                                            # If None, then unlimited number of leaf nodes\n","                                random_state= 123\n","                                )\n","rf_clf.fit(X_train, y_train)\n","y_pred_rf = rf_clf.predict(X_test)\n","# YOUR CODE HERE to show accuracy_score for (y_test, y_pred_rf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXiMLJptPByW"},"source":["The following `BaggingClassifier` is roughly equivalent to the previous `RandomForestClassifier`:"]},{"cell_type":"code","metadata":{"id":"9p4zqAifPByW"},"source":["dt = DecisionTreeClassifier(splitter=\"random\",               # strategy used to choose the split at each node. \n","                                                             # \"best\" to choose the best split and \n","                                                             # \"random\" to choose the best random split\n","                            max_leaf_nodes=16                 \n","                            )\n","# YOUR CODE HERE to create 'bag_clf' using BaggingClassifier() with (base_estimator=dt, n_estimators=300, max_samples=1.0, bootstrap=True, random_state= 123)\n","bag_clf.fit(X_train, y_train)\n","y_pred_bag = bag_clf.predict(X_test)\n","print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred_bag))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AjYFToyYPByW"},"source":["#### Feature Importance"]},{"cell_type":"markdown","metadata":{"id":"IOnjb1urPByW"},"source":["Random Forests make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it.\n","\n","Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. We can access the result using the `feature_importances_` instance variable. \n","\n","For example, the following code trains a RandomForestClassifier on the Iris dataset and outputs each feature’s importance."]},{"cell_type":"markdown","metadata":{"id":"RO0SbNOrPByW"},"source":["**Exercise 5:** Extract the important features from Iris dataset using RandomForestClassifier."]},{"cell_type":"code","metadata":{"id":"nT42iieJPByW"},"source":["# Load iris dataset\n","iris = datasets.load_iris()\n","# Prediction and target features\n","prediction_features = iris[\"data\"]\n","# YOUR CODE HERE to create 'target_feature' using iris[\"target\"]\n","\n","# Instantiate Random Forest classifier with n_estimators=500\n","rf_clf = RandomForestClassifier(n_estimators=500, \n","                                random_state=123\n","                                )\n","# Fit on iris data\n","rf_clf.fit(prediction_features, target_feature)\n","\n","for name, score in zip(iris[\"feature_names\"], rf_clf.feature_importances_):\n","   print(name, score)\n","\n","# Plot Feature importances\n","plt.bar(iris[\"feature_names\"], rf_clf.feature_importances_ * 100)\n","plt.xticks(rotation=45, fontsize=12)\n","plt.ylabel(\"Feature Importance (%)\", fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Mv3_yeIPByW"},"source":["From the above result, it seems that the most important features are the petal length (45%) and petal width (42%), while sepal length and sepal width are rather unimportant in comparison (9% and 2%, respectively).\n","\n","Let's see one exercise on real world dataset."]},{"cell_type":"markdown","metadata":{"id":"4PkR_SNFPByX"},"source":["**Exercise 6:** Using `heart_failure_clinical_records_dataset`, build a classifier with Random forest to predict `DEATH_EVENT`. Also, extract the important features from this dataset and see if it affects the accuracy if only these features as used for prediction."]},{"cell_type":"markdown","metadata":{"id":"CLSTyQWTPByX"},"source":["[Heart failure clinical records dataset](https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records) contains the medical records of 299 patients who had heart failure, collected during their follow-up period, where each patient profile has 13 clinical features given as:\n","\n","- **age**: age of the patient (years)\n","- **anaemia**: decrease of red blood cells or hemoglobin (boolean)\n","- **high_blood_pressure**: if the patient has hypertension (boolean)\n","- **creatinine_phosphokinase (CPK)**: level of the CPK enzyme in the blood (mcg/L)\n","- **diabetes**: if the patient has diabetes (boolean)\n","- **ejection_fraction**: percentage of blood leaving the heart at each contraction (percentage)\n","- **platelets**: platelets in the blood (kiloplatelets/mL)\n","- **sex**: woman or man (binary)\n","- **serum_creatinine**: level of serum creatinine in the blood (mg/dL)\n","- **serum_sodium**: level of serum sodium in the blood (mEq/L)\n","- **smoking**: if the patient smokes or not (boolean)\n","- **time**: follow-up period (days)\n","- **DEATH_EVENT**: if the patient deceased during the follow-up period (boolean)"]},{"cell_type":"code","metadata":{"id":"5h-P2qKUPByX"},"source":["# Load dataset\n","df = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBNbddNbPByX"},"source":["# Shape of dataset\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xh6t9wF8PByX"},"source":["# Check for missing values\n","df.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0mjU93xPByY"},"source":["# Checking for outliers\n","df.boxplot()\n","plt.xticks(rotation=90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sqt1eZHiPByY"},"source":["# Handing outliers\n","outlier_colms = ['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']\n","df1 = df.copy()\n","\n","def handle_outliers(df, colm):\n","    '''Change the values of outlier to upper and lower whisker values '''\n","    q1 = df.describe()[colm].loc[\"25%\"]\n","    q3 = df.describe()[colm].loc[\"75%\"]\n","    iqr = q3 - q1\n","    lower_bound = q1 - (1.5 * iqr)\n","    upper_bound = q3 + (1.5 * iqr)\n","    for i in range(len(df)):\n","        if df.loc[i,colm] > upper_bound:\n","            df.loc[i,colm]= upper_bound\n","        if df.loc[i,colm] < lower_bound:\n","            df.loc[i,colm]= lower_bound\n","    # YOUR CODE HERE to return df\n","    \n","for colm in outlier_colms:\n","    df1 = handle_outliers(df1, colm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YQKHdS9PByY"},"source":["# Recheck for outliers\n","# YOUR CODE HERE for boxplot using 'df1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWvPXkGtPByY"},"source":["# Split dataset into training and testing set, considering all features for prediction\n","X = df1.iloc[:, :-1].values\n","y = df1['DEATH_EVENT'].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state= 123)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cpeAz2v7PByY"},"source":["# Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=400, random_state=123)\n","# YOUR CODE HERE to fit 'rf' on (X_train, y_train)\n","test_acc = accuracy_score(y_test, rf.predict(X_test))\n","print(\"Testing accuracy: \", test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xp8Ml10iPByY"},"source":["# Feature Importance plot\n","plt.bar(df1.columns[:-1], rf.feature_importances_ * 100)\n","plt.xticks(rotation=90, fontsize = 12)\n","plt.ylabel(\"Feature Importance (Percentage)\", size = 10)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aR-HF4IPByZ"},"source":["# Considering features with importance > 0.08\n","colms = df1.columns[:-1][rf.feature_importances_ > 0.08]\n","# YOUR CODE HERE to show 'colms'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cr-9L-8pPByZ"},"source":["# Split dataset into training and testing set, considering selected features only\n","X1 = df1[colms].values\n","y1 = df1['DEATH_EVENT'].values\n","X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.3, stratify= y1, random_state= 123)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHTgVmg7PByZ"},"source":["# Random Forest classifier\n","rf_ = RandomForestClassifier(n_estimators=400, random_state=123)\n","rf_.fit(X_train, y_train)\n","# YOUR CODE HERE to create 'test_acc'\n","print(\"Testing accuracy: \", test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kuPKeJ76PByZ"},"source":["From the above results, we can see that accuracy is increased from 0.88 to 0.91 after training the Random Forest model on extracted features."]},{"cell_type":"markdown","metadata":{"id":"4MxYLKE2PByZ"},"source":["### Comparison between Bagging and Random Forest"]},{"cell_type":"markdown","metadata":{"id":"eEo4zt4DPByZ"},"source":["|  Bagging  |  Random Forest  | \n","|:--------------|:-----------|\n","|  One of the simplest ensemble-based algorithms applied to enhance predictive accuracy  | Enhanced version of bagging, ensemble of decision trees trained with a bagging mechanism |\n","|  We can specify the base estimator  | Uses Decision Tree as base estimator |\n","|  Every Decision Tree is taking all the features  | Each Decision Tree works on the subset of the features | \n","|  Concept is to train a bunch of unpruned decision trees on different random subsets of the training data, sampling with replacement  | Concept is to build multiple decision trees and aggregate them to get an accurate result with as little bias as possible | \n","|  Improves the accuracy and stability of ML models using ensemble learning and reduces the complexity of overfitting models  | It is very robust against overfitting and good with unbalanced and missing data | "]},{"cell_type":"markdown","metadata":{"id":"3irDW2MnPByZ"},"source":["### Theory Questions"]},{"cell_type":"markdown","metadata":{"id":"IZDnm3gvPBya"},"source":["1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n","\n"," If you have trained five different models and they all achieve 95% precision, you can try combining them into a voting ensemble, which will often give you even better results. It works better if the models are very different (e.g., an SVM classifier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are trained on different training instances (that’s the whole point of bagging and pasting ensembles), but if not this will still be effective as long as the models are very different.\n","\n","\n","2. What is the difference between hard and soft voting classifiers?\n","\n"," A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets the most votes. A soft voting classifier computes the average estimated class probability for each class and picks the class with the highest probability. This gives high-confidence votes more weight and often performs better, but it works only if every classifier is able to estimate class probabilities (e.g., for the SVM classifiers in Scikit-Learn you must set probability=True).\n","\n","\n","3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, or Random Forests?\n","\n"," It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the same reason. However, each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily sequential, and you will not gain anything by distributing training across multiple servers.\n","\n","\n","4. What is the benefit of out-of-bag evaluation?\n","\n"," With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated\n","using instances that it was not trained on (they were held out). This makes it possible to have a fairly unbiased evaluation of the ensemble without the need for an additional validation set. Thus, you have more instances available for training, and your ensemble can perform slightly better.\n","\n","\n","5. What makes Extra-Trees more random than regular Random Forests? How can\n","this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n","\n"," When you are growing a tree in a Random Forest, only a random subset of the\n","features is considered for splitting at each node. This is true as well for ExtraTrees, but they go one step further: rather than searching for the best possible thresholds, like regular Decision Trees do, they use random thresholds for each feature. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform better. Moreover, since Extra-Trees don’t search for the best possible thresholds, they are much faster to train than Random Forests. However, they are neither faster nor slower than Random Forests when making predictions. You can create an Extra-Trees classifier using Scikit-Learn’s `ExtraTreesClassifier` class. Its API is identical to the `RandomForestClassifier` class. "]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["#@title Select the False statement w.r.t Random Forest ensemble method: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"It uses bagging and feature randomness when building each individual tree\", \"It randomly selects features from total features, to get the best split point\", \"It is not possible to speed up training of a random forest ensemble by distributing it across multiple servers\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}